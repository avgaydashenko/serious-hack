{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(y_true, y_pred):\n",
    "    print('F1: {}'.format(f1_score(y_true, y_pred, average=None)))\n",
    "    print('Precision: {}'.format(precision_score(y_true, y_pred, average=None)))\n",
    "    print('Recall: {}'.format(recall_score(y_true, y_pred, average=None)))\n",
    "    \n",
    "def draw_importance(columns, importances):\n",
    "    order = np.argsort(importances)\n",
    "    objects = columns[order]\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = np.array(importances)[order]\n",
    "\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(y_test):\n",
    "    with open('y_test.csv', 'w') as f:\n",
    "        for y in y_test:\n",
    "            f.write('{}\\n'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../train.csv')\n",
    "test = pd.read_csv('../test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['zipcode'])\n",
    "test = test.drop(columns=['zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless = ['init_equities']\n",
    "train = train.drop(columns=useless)\n",
    "test = test.drop(columns=useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['region', 'company_type', 'company_name', 'inn', 'incorporation_year', \n",
    "                'okved_osn_code', 'init_equity_types', 'purpose', 'other_reasons_for_check', \n",
    "                'check_date', 'type', 'kpp', 'type_nasel_punkt', 'name_nasel_punkt', 'index']\n",
    "\n",
    "columns = list(train.columns)\n",
    "cat_features_idx = [columns.index(feature) for feature in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[cat_features] = train[cat_features].fillna(\"none\")\n",
    "train = train.fillna(0)\n",
    "\n",
    "test[cat_features] = test[cat_features].fillna(\"none\")\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change columns type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in cat_features:\n",
    "    train[name] = train[name].astype(str)\n",
    "    test[name] = test[name].astype(str)\n",
    "    \n",
    "train['risk_category'] = train['risk_category'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "\n",
    "nums = 30000\n",
    "inns = train.inn.unique()\n",
    "np.random.shuffle(inns)\n",
    "inns = set(inns[:nums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = np.array([row.inn in inns for index, row in train.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_val = ~mask_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['inn']).as_matrix()[:, :-1]\n",
    "y = train.drop(columns=['inn']).as_matrix()[:, -1].astype(int)\n",
    "\n",
    "X_train = X[mask_train]\n",
    "y_train = y[mask_train]\n",
    "\n",
    "X_val = X[mask_val]\n",
    "y_val = y[mask_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_idx = np.arange(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train, cat_features_idx)\n",
    "val_pool = Pool(X_val, y_val, cat_features_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=90, \n",
    "                           learning_rate=0.08, \n",
    "                           depth=5,\n",
    "                           loss_function='MultiClass',\n",
    "                           eval_metric='TotalF1',\n",
    "                           rsm=0.8,\n",
    "                           thread_count=5, \n",
    "                           logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_pool, eval_set=val_pool, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train).squeeze().astype(int)\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val).squeeze().astype(int)\n",
    "f1_score(y_val, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array(cat_features[:3] + cat_features[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_importance(z, model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.as_matrix()[:, :-1]\n",
    "y_test = model.predict(X_test).squeeze().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BagBoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = X.shape[0]\n",
    "bags = [np.random.choice(list(range(rows)), size=rows, replace=True) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0] + list(np.log(len(y) / np.unique(y, return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for mask in tqdm(bags):\n",
    "    pool = Pool(X[mask], y[mask], cat_features_idx)\n",
    "    \n",
    "    model = CatBoostClassifier(iterations=90, \n",
    "                           learning_rate=0.08, \n",
    "                           depth=5,\n",
    "                           loss_function='MultiClass',\n",
    "                           eval_metric='TotalF1',\n",
    "                           rsm=0.8,\n",
    "                           thread_count=20, \n",
    "                           logging_level='Silent', class_weights=w)\n",
    "    \n",
    "    model.fit(pool, eval_set=val_pool, plot=False)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.array([model.predict_proba(X_train) for model in models])\n",
    "y_pred = np.argsort(np.sum(prob, axis=0))[:, -1]\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.array([model.predict_proba(X_val) for model in models])\n",
    "y_pred = np.argsort(np.sum(prob, axis=0))[:, -1]\n",
    "f1_score(y_val, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns=['inn']).as_matrix()[:, :-1]\n",
    "\n",
    "prob = np.array([model.predict_proba(X_test) for model in models])\n",
    "y_test = np.argsort(np.sum(prob, axis=0))[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
