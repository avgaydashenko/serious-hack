{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../train.csv')\n",
    "test = pd.read_csv('../test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['region', 'company_type', 'company_name', 'inn', 'incorporation_year',\n",
    "           'okved_osn_code', 'init_equities', 'init_equity_types', 'purpose',\n",
    "           'other_reasons_for_check', 'check_date', 'type', 'kpp',\n",
    "           'type_nasel_punkt', 'name_nasel_punkt', 'index', 'zipcode', 'risk_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[columns]\n",
    "test = test[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless = ['init_equities']\n",
    "train = train.drop(columns=useless)\n",
    "test = test.drop(columns=useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['region', 'company_type', 'company_name', 'inn', 'incorporation_year', \n",
    "                'okved_osn_code', 'init_equity_types', 'purpose', 'other_reasons_for_check', \n",
    "                'check_date', 'type', 'kpp', 'type_nasel_punkt', 'name_nasel_punkt', 'index', 'zipcode']\n",
    "\n",
    "columns = list(train.columns)\n",
    "cat_features_idx = [columns.index(feature) for feature in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[cat_features] = train[cat_features].fillna(\"none\")\n",
    "train = train.fillna(0)\n",
    "\n",
    "test[cat_features] = test[cat_features].fillna(\"none\")\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change columns type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in cat_features:\n",
    "    train[name] = train[name].astype(str)\n",
    "    test[name] = test[name].astype(str)\n",
    "    \n",
    "train['risk_category'] = train['risk_category'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "\n",
    "nums = 30000\n",
    "inns = train.inn.unique()\n",
    "np.random.shuffle(inns)\n",
    "inns = set(inns[:nums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = np.array([row.inn in inns for index, row in train.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_val = ~mask_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.as_matrix()[:, :-1]\n",
    "y = train.as_matrix()[:, -1].astype(int)\n",
    "\n",
    "X_train = X[mask_train]\n",
    "y_train = y[mask_train]\n",
    "\n",
    "X_val = X[mask_val]\n",
    "y_val = y[mask_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(X, y, clazz):\n",
    "    indx_pos = (y == clazz)\n",
    "    indx_neg = np.random.choice(np.arange(X.shape[0])[~indx_pos], size=sum(indx_pos), replace=False)\n",
    "    X_new = np.concatenate((X[indx_pos], X[indx_neg]))\n",
    "    y_new = np.concatenate((np.ones(sum(indx_pos)), np.zeros(sum(indx_pos))))\n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, clazz, plot=True, logging_level='Silent'):\n",
    "    X_train_, y_train_ = generate_data(X_train, y_train, clazz)\n",
    "    X_val_, y_val_ = generate_data(X_val, y_val, clazz)\n",
    "    \n",
    "    model = CatBoostClassifier(iterations=60, \n",
    "                           learning_rate=0.1, \n",
    "                           depth=3,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='F1',\n",
    "                           rsm=0.8,\n",
    "                           thread_count=5)\n",
    "    \n",
    "    train_pool_ = Pool(X_train_, y_train_, cat_features_idx)\n",
    "    val_pool_ = Pool(X_val_, y_val_, cat_features_idx)\n",
    "\n",
    "    model.fit(train_pool_, eval_set=val_pool_, plot=plot, logging_level=logging_level)\n",
    "    \n",
    "    y_pred = model.predict(X_train_).squeeze().astype(int)\n",
    "    print('Train')\n",
    "    print('F1: {}'.format(f1_score(y_train_, y_pred)))\n",
    "    print('Precision: {}'.format(precision_score(y_train_, y_pred)))\n",
    "    print('Recall: {}'.format(recall_score(y_train_, y_pred)))\n",
    "    \n",
    "    y_pred = model.predict(X_val_).squeeze().astype(int)\n",
    "    print('\\nValidate')\n",
    "    print('F1: {}'.format(f1_score(y_val_, y_pred)))\n",
    "    print('Precision: {}'.format(precision_score(y_val_, y_pred)))\n",
    "    print('Recall: {}'.format(recall_score(y_val_, y_pred)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = train_model(X_train, y_train, X_val, y_val, clazz=1, plot=False, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = train_model(X_train, y_train, X_val, y_val, clazz=2, plot=False, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = train_model(X_train, y_train, X_val, y_val, clazz=3, plot=False, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = train_model(X_train, y_train, X_val, y_val, clazz=4, plot=False, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = train_model(X_train, y_train, X_val, y_val, clazz=5, plot=False, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = train_model(X_train, y_train, X_val, y_val, clazz=6, plot=False, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2, model3, model4, model5, model6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(models, X):\n",
    "    return np.array([model.predict_proba(X)[:, 1] for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = get_prob(models, X_train)\n",
    "y_pred = np.argsort(prob, axis=0)[-1] + 1\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = get_prob(models, X_val)\n",
    "y_pred = np.argsort(prob, axis=0)[-1] + 1\n",
    "f1_score(y_val, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train, cat_features_idx)\n",
    "val_pool = Pool(X_val, y_val, cat_features_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=60, \n",
    "                           learning_rate=0.1, \n",
    "                           depth=5,\n",
    "                           loss_function='MultiClass',\n",
    "                           eval_metric='TotalF1',\n",
    "                           rsm=0.8,\n",
    "                           thread_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_pool, eval_set=val_pool, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train).squeeze().astype(int)\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val).squeeze().astype(int)\n",
    "f1_score(y_val, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_importance(train.columns[:-1], model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.as_matrix()[:, :-1]\n",
    "y_test = model.predict(X_test).squeeze().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BagBoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = X_train.shape[0]\n",
    "bags = [np.random.choice(list(range(rows)), size=rows, replace=True) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for mask in tqdm(bags):\n",
    "    pool = Pool(X_train[mask], y_train[mask], cat_features_idx)\n",
    "    \n",
    "    model = CatBoostClassifier(iterations=60, \n",
    "                               learning_rate=0.1, \n",
    "                               depth=5,\n",
    "                               loss_function='MultiClass',\n",
    "                               eval_metric='TotalF1',\n",
    "                               rsm=0.8,\n",
    "                               thread_count=20,\n",
    "                               logging_level='Silent')\n",
    "    \n",
    "    model.fit(pool, eval_set=val_pool, plot=False)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.array([model.predict_proba(X_train) for model in models])\n",
    "y_pred = np.argsort(np.sum(prob, axis=0))[:, -1]\n",
    "f1_score(y_train, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.array([model.predict_proba(X_val) for model in models])\n",
    "y_pred = np.argsort(np.sum(prob, axis=0))[:, -1]\n",
    "f1_score(y_val, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.as_matrix()[:, :-1]\n",
    "\n",
    "prob = np.array([model.predict_proba(X_test) for model in models])\n",
    "y_test = np.argsort(np.sum(prob, axis=0))[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(y_true, y_pred):\n",
    "    print('F1: {}'.format(f1_score(y_true, y_pred, average=None)))\n",
    "    print('Precision: {}'.format(precision_score(y_true, y_pred, average=None)))\n",
    "    print('Recall: {}'.format(recall_score(y_true, y_pred, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_importance(columns, importances):\n",
    "    order = np.argsort(importances)\n",
    "    objects = columns[order]\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = np.array(importances)[order]\n",
    "\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(y_test):\n",
    "    with open('y_test.csv', 'w') as f:\n",
    "        for y in y_test:\n",
    "            f.write('{}\\n'.format(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
